{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import json\nimport glob\nimport numpy as np\nimport pandas as pd\nimport csv\nimport string\nimport re\nimport math\nimport sklearn as sk\nimport nltk\nimport heapq\nimport matplotlib.pyplot as plt\nimport pickle","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-05-08T19:15:17.367983Z","iopub.execute_input":"2022-05-08T19:15:17.368198Z","iopub.status.idle":"2022-05-08T19:15:17.373788Z","shell.execute_reply.started":"2022-05-08T19:15:17.368171Z","shell.execute_reply":"2022-05-08T19:15:17.372986Z"},"trusted":true},"execution_count":71,"outputs":[]},{"cell_type":"markdown","source":"**IMPORTING AND GETTING ALL LIBRARIES**","metadata":{}},{"cell_type":"markdown","source":"Link to download dataset: https://github.com/avadhutshelar/SmartLawDataset","metadata":{}},{"cell_type":"code","source":"from sklearn.pipeline import Pipeline\nfrom sklearn import svm\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import learning_curve\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.model_selection import train_test_split\nfrom nltk.corpus import stopwords\nfrom nltk import pos_tag\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import learning_curve\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import StratifiedKFold\nimport seaborn as sns","metadata":{"execution":{"iopub.status.busy":"2022-05-08T19:15:17.375150Z","iopub.execute_input":"2022-05-08T19:15:17.375582Z","iopub.status.idle":"2022-05-08T19:15:17.386550Z","shell.execute_reply.started":"2022-05-08T19:15:17.375540Z","shell.execute_reply":"2022-05-08T19:15:17.385961Z"},"trusted":true},"execution_count":72,"outputs":[]},{"cell_type":"code","source":"#LOCATE EXACT LOCATION OF ALL JSON FILES IN KAGGLE LOCAL SETUP\ntrain_files = glob.glob(\"../input/part-2/SmartLawDataset-main/json/*.json\")","metadata":{"execution":{"iopub.status.busy":"2022-05-08T19:15:17.387597Z","iopub.execute_input":"2022-05-08T19:15:17.387827Z","iopub.status.idle":"2022-05-08T19:15:17.405817Z","shell.execute_reply.started":"2022-05-08T19:15:17.387799Z","shell.execute_reply":"2022-05-08T19:15:17.405000Z"},"trusted":true},"execution_count":73,"outputs":[]},{"cell_type":"markdown","source":"**EXTRACTING THE REQUIRED TEXT AND GETTING THEM INTO PROPER FORMAT**","metadata":{}},{"cell_type":"code","source":"#EXTRACT INDIVIDUAL FILES AND GET THEM INTO SINGLE DATAFRAME\ndf_final=pd.DataFrame()\nfor file in train_files:\n    with open(file) as f1: \n        data=json.load(f1) #ALL DATA FROM FILE LOADED INTO DATA\n        df=pd.DataFrame([data]) #CREATE DATAFRAME FOR SINGLE DATA\n        df_final=pd.concat([df_final,df]) #MERGE ALL INDIVIDUAL DATAFRAME INTO SINGLE DATAFRAME\n#MAIN AIM IS TO GENERATE ML ALGO TO DETERMINE ARGUMENT TEXT AND ARGUMENT BY SO REMOVE REST\ndf_final=df_final.drop(['header','background','order','footer','annotationProcessingStage','annotationProcessingStageAnnotations','processedText'],axis=1)\n#AXIS=1 REMOVE COLUMNS","metadata":{"execution":{"iopub.status.busy":"2022-05-08T19:15:17.407226Z","iopub.execute_input":"2022-05-08T19:15:17.407630Z","iopub.status.idle":"2022-05-08T19:15:17.718789Z","shell.execute_reply.started":"2022-05-08T19:15:17.407600Z","shell.execute_reply":"2022-05-08T19:15:17.717915Z"},"trusted":true},"execution_count":74,"outputs":[]},{"cell_type":"code","source":"data1=[]\ndata2=[]\ndf_final1=pd.DataFrame();\nfor j in df_final['arguments']:\n    for k in j:\n        s=k['text'][3:]\n        data1.append(s.strip())\n        data2.append(k['argumentBy'])\ndf_final1['text']=data1\ndf_final1['by']=data2\n#Use df_final1 dataframe only for further processes and not csv files to ensure encoding is in proper format\nprint(df_final1)","metadata":{"execution":{"iopub.status.busy":"2022-05-08T19:15:17.720227Z","iopub.execute_input":"2022-05-08T19:15:17.720514Z","iopub.status.idle":"2022-05-08T19:15:17.735900Z","shell.execute_reply.started":"2022-05-08T19:15:17.720475Z","shell.execute_reply":"2022-05-08T19:15:17.735004Z"},"trusted":true},"execution_count":75,"outputs":[]},{"cell_type":"code","source":"s =  set(string.punctuation)\ns.add('\\xad')\nfor index, row in df_final1.iterrows():\n    for x in row['text']:\n        row['text']=row['text'].lower()\n#         print(row['text'])\n        if x in s or re.search(r'-?\\d+', x): \n            row['text']=row['text'].replace(x,\"\")\n#Now we have proper dataset after removing unwanted punctuations etc\nbag_of_words=[]\nlos=[]\nfor item in df_final1['text']:\n    los.append(item)\n    for word in item.split():\n        bag_of_words.append(word)","metadata":{"execution":{"iopub.status.busy":"2022-05-08T19:15:17.737444Z","iopub.execute_input":"2022-05-08T19:15:17.739084Z","iopub.status.idle":"2022-05-08T19:15:23.934829Z","shell.execute_reply.started":"2022-05-08T19:15:17.739049Z","shell.execute_reply":"2022-05-08T19:15:23.933964Z"},"trusted":true},"execution_count":76,"outputs":[]},{"cell_type":"markdown","source":"Wordset containing all words.\ncreate dic for freq of all words in doc.\ncalculate tf.\n","metadata":{}},{"cell_type":"markdown","source":"**CREATING VECTORS FOR TEXTS IN ORDER TO GIVE THEM AS INPUT TO ML MODEL**","metadata":{}},{"cell_type":"code","source":"dict_1={}\nfor index,row in df_final1.iterrows():\n    for word in row['text'].split():\n        if word in dict_1:\n            dict_1[word]+=1\n        else:\n            dict_1[word]=1","metadata":{"execution":{"iopub.status.busy":"2022-05-08T19:15:23.936276Z","iopub.execute_input":"2022-05-08T19:15:23.936518Z","iopub.status.idle":"2022-05-08T19:15:23.997683Z","shell.execute_reply.started":"2022-05-08T19:15:23.936488Z","shell.execute_reply":"2022-05-08T19:15:23.996953Z"},"trusted":true},"execution_count":77,"outputs":[]},{"cell_type":"code","source":"# Calculate term_freq\ndef compute_term_frequency(word_dictionary, bag_of_words):\n    term_frequency_dictionary = {}\n    length_of_bag_of_words = len(bag_of_words)\n\n    for word, count in word_dictionary.items():\n        term_frequency_dictionary[word] = count / float(length_of_bag_of_words)\n\n    return term_frequency_dictionary","metadata":{"execution":{"iopub.status.busy":"2022-05-08T19:15:23.998836Z","iopub.execute_input":"2022-05-08T19:15:23.999229Z","iopub.status.idle":"2022-05-08T19:15:24.004769Z","shell.execute_reply.started":"2022-05-08T19:15:23.999194Z","shell.execute_reply":"2022-05-08T19:15:24.003984Z"},"trusted":true},"execution_count":78,"outputs":[]},{"cell_type":"code","source":"dict_2={}\ndict_3={}\nfor item in los:\n    for word in item.split():\n        if word in dict_2 and word not in dict_3:\n            dict_2[word]+=1\n            dict_3[word]=1\n        else:\n            if word not in dict_2:\n                dict_2[word]=1\n                dict_3[word]=1\n    dict_3.clear()","metadata":{"execution":{"iopub.status.busy":"2022-05-08T19:15:24.006016Z","iopub.execute_input":"2022-05-08T19:15:24.006714Z","iopub.status.idle":"2022-05-08T19:15:24.051162Z","shell.execute_reply.started":"2022-05-08T19:15:24.006663Z","shell.execute_reply":"2022-05-08T19:15:24.050073Z"},"trusted":true},"execution_count":79,"outputs":[]},{"cell_type":"code","source":"#Calculate inverse doc_freq\ndef compute_inverse_document_frequency(full_doc_list):\n    idf_dict = {}\n    length_of_doc_list = len(full_doc_list)\n\n    idf_dict = dict.fromkeys(full_doc_list.keys(), 0)\n    for word, value in idf_dict.items():\n        idf_dict[word] = math.log(length_of_doc_list / (float(value) + 1))\n\n    return idf_dict\n\nfinal_idf_dict = compute_inverse_document_frequency(dict_2)","metadata":{"execution":{"iopub.status.busy":"2022-05-08T19:15:24.052411Z","iopub.execute_input":"2022-05-08T19:15:24.052832Z","iopub.status.idle":"2022-05-08T19:15:24.067951Z","shell.execute_reply.started":"2022-05-08T19:15:24.052801Z","shell.execute_reply":"2022-05-08T19:15:24.066968Z"},"trusted":true},"execution_count":80,"outputs":[]},{"cell_type":"code","source":"#Create a TFIDF vectorizer to generate text entered into vector form to be given as input to Machine Learning model\nvectorizer = TfidfVectorizer()\nvectors = vectorizer.fit_transform(los)\nfeature_names = vectorizer.get_feature_names_out() #Extract the feature names as columns for the texts\ndense = vectors.todense()\ndenselist = dense.tolist()\ndf_end = pd.DataFrame(denselist, columns=feature_names)\ndf_end['By']=df_final1['by']","metadata":{"execution":{"iopub.status.busy":"2022-05-08T19:15:24.069482Z","iopub.execute_input":"2022-05-08T19:15:24.070267Z","iopub.status.idle":"2022-05-08T19:15:24.874792Z","shell.execute_reply.started":"2022-05-08T19:15:24.070230Z","shell.execute_reply":"2022-05-08T19:15:24.873887Z"},"trusted":true},"execution_count":81,"outputs":[]},{"cell_type":"code","source":"y=df_end.By\nx=df_end[feature_names]\n# Setting up x and y coordinates\n# x.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-08T19:15:24.876024Z","iopub.execute_input":"2022-05-08T19:15:24.876270Z","iopub.status.idle":"2022-05-08T19:15:24.889505Z","shell.execute_reply.started":"2022-05-08T19:15:24.876241Z","shell.execute_reply":"2022-05-08T19:15:24.888860Z"},"trusted":true},"execution_count":82,"outputs":[]},{"cell_type":"markdown","source":"**PERFORMING TRAIN TEST SPLIT AND EVALUATING ACCURACIES FOR VARIOUS MODELS**","metadata":{}},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(x, y, test_size = 0.25, random_state = 0)\n# check the shape of X_train and X_test\nX_train.shape, X_test.shape","metadata":{"execution":{"iopub.status.busy":"2022-05-08T19:15:24.890874Z","iopub.execute_input":"2022-05-08T19:15:24.891645Z","iopub.status.idle":"2022-05-08T19:15:24.914479Z","shell.execute_reply.started":"2022-05-08T19:15:24.891595Z","shell.execute_reply":"2022-05-08T19:15:24.913878Z"},"trusted":true},"execution_count":83,"outputs":[]},{"cell_type":"code","source":"gnb = GaussianNB()\ngnb.fit(X_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2022-05-08T19:15:24.915811Z","iopub.execute_input":"2022-05-08T19:15:24.916272Z","iopub.status.idle":"2022-05-08T19:15:24.985434Z","shell.execute_reply.started":"2022-05-08T19:15:24.916230Z","shell.execute_reply":"2022-05-08T19:15:24.984818Z"},"trusted":true},"execution_count":84,"outputs":[]},{"cell_type":"code","source":"y_pred = gnb.predict(X_test)\ny_pred_train = gnb.predict(X_train)","metadata":{"execution":{"iopub.status.busy":"2022-05-08T19:15:24.987002Z","iopub.execute_input":"2022-05-08T19:15:24.987534Z","iopub.status.idle":"2022-05-08T19:15:25.099671Z","shell.execute_reply.started":"2022-05-08T19:15:24.987492Z","shell.execute_reply":"2022-05-08T19:15:25.098810Z"},"trusted":true},"execution_count":85,"outputs":[]},{"cell_type":"code","source":"print('Model accuracy score using TF_IDF- GNB for predicting argument by: {0:0.4f}'. format(accuracy_score(y_test, y_pred)))","metadata":{"execution":{"iopub.status.busy":"2022-05-08T19:15:25.101260Z","iopub.execute_input":"2022-05-08T19:15:25.101573Z","iopub.status.idle":"2022-05-08T19:15:25.108817Z","shell.execute_reply.started":"2022-05-08T19:15:25.101521Z","shell.execute_reply":"2022-05-08T19:15:25.107809Z"},"trusted":true},"execution_count":86,"outputs":[]},{"cell_type":"code","source":"from sklearn.svm import SVC\nsvclassifier = SVC(kernel='linear')\nsvclassifier.fit(X_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2022-05-08T19:15:25.110124Z","iopub.execute_input":"2022-05-08T19:15:25.110432Z","iopub.status.idle":"2022-05-08T19:15:25.450541Z","shell.execute_reply.started":"2022-05-08T19:15:25.110400Z","shell.execute_reply":"2022-05-08T19:15:25.449751Z"},"trusted":true},"execution_count":87,"outputs":[]},{"cell_type":"code","source":"y_pred = svclassifier.predict(X_test)","metadata":{"execution":{"iopub.status.busy":"2022-05-08T19:15:25.451531Z","iopub.execute_input":"2022-05-08T19:15:25.451756Z","iopub.status.idle":"2022-05-08T19:15:25.572228Z","shell.execute_reply.started":"2022-05-08T19:15:25.451721Z","shell.execute_reply":"2022-05-08T19:15:25.570790Z"},"trusted":true},"execution_count":88,"outputs":[]},{"cell_type":"code","source":"print('Model accuracy score using TF_IDF- SVC for predicing argument by: {0:0.4f}'. format(accuracy_score(y_test, y_pred)))","metadata":{"execution":{"iopub.status.busy":"2022-05-08T19:15:25.573959Z","iopub.execute_input":"2022-05-08T19:15:25.574380Z","iopub.status.idle":"2022-05-08T19:15:25.583601Z","shell.execute_reply.started":"2022-05-08T19:15:25.574331Z","shell.execute_reply":"2022-05-08T19:15:25.582782Z"},"trusted":true},"execution_count":89,"outputs":[]},{"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier  \nclassifier= KNeighborsClassifier(n_neighbors=5, metric='minkowski', p=2 )  \nclassifier.fit(X_train, y_train)  ","metadata":{"execution":{"iopub.status.busy":"2022-05-08T19:15:25.585392Z","iopub.execute_input":"2022-05-08T19:15:25.585898Z","iopub.status.idle":"2022-05-08T19:15:25.645979Z","shell.execute_reply.started":"2022-05-08T19:15:25.585853Z","shell.execute_reply":"2022-05-08T19:15:25.643734Z"},"trusted":true},"execution_count":90,"outputs":[]},{"cell_type":"code","source":"y_pred= classifier.predict(X_test) ","metadata":{"execution":{"iopub.status.busy":"2022-05-08T19:15:25.647411Z","iopub.execute_input":"2022-05-08T19:15:25.648158Z","iopub.status.idle":"2022-05-08T19:15:25.724062Z","shell.execute_reply.started":"2022-05-08T19:15:25.648111Z","shell.execute_reply":"2022-05-08T19:15:25.722743Z"},"trusted":true},"execution_count":91,"outputs":[]},{"cell_type":"code","source":"print('Model accuracy score using TF_IDF- KNN for predicting argument by: {0:0.4f}'. format(accuracy_score(y_test, y_pred)))","metadata":{"execution":{"iopub.status.busy":"2022-05-08T19:15:25.725921Z","iopub.execute_input":"2022-05-08T19:15:25.726287Z","iopub.status.idle":"2022-05-08T19:15:25.740004Z","shell.execute_reply.started":"2022-05-08T19:15:25.726242Z","shell.execute_reply":"2022-05-08T19:15:25.734732Z"},"trusted":true},"execution_count":92,"outputs":[]},{"cell_type":"markdown","source":"**UNDERSTANDING THE ACCURACIES AND VALIDATION LOSSES AS DATASET SIZE INCREASES TO KNOW OVERFITTING IF ANY**","metadata":{}},{"cell_type":"code","source":"def learn_curve(X,y,c):\n    le = preprocessing.LabelEncoder() # Label encoding the target\n    sc = preprocessing.StandardScaler() # Scaling the input features\n    y = le.fit_transform(y)#Label Encoding the target\n    log_reg = LogisticRegression(max_iter=200,random_state=11,C=c)\n    lr = Pipeline(steps=(['scaler',sc],\n                        ['classifier',log_reg]))\n    \n    \n    cv = StratifiedKFold(n_splits=5,random_state=11,shuffle=True) # Creating a StratifiedKFold object with 5 folds\n    cv_scores = cross_val_score(lr,X,y,scoring=\"accuracy\",cv=cv) # Storing the CV scores (accuracy) of each fold\n    lr.fit(X,y) # Fitting the model\n    train_score = lr.score(X,y) # Scoring the model on train set\n    #Building the learning curve\n    train_size,train_scores,test_scores = learning_curve(estimator=lr,X=X,y=y,cv=cv,scoring=\"accuracy\",random_state=11)\n    train_scores = 1-np.mean(train_scores,axis=1)#converting the accuracy score to misclassification rate\n    test_scores = 1-np.mean(test_scores,axis=1)#converting the accuracy score to misclassification rate\n    lc = pd.DataFrame({\"Training_size\":train_size,\"Training_loss\":train_scores,\"Validation_loss\":test_scores}).melt(id_vars=\"Training_size\")\n    return {\"cv_scores\":cv_scores,\n           \"train_score\":train_score,\n           \"learning_curve\":lc}","metadata":{"execution":{"iopub.status.busy":"2022-05-08T19:15:25.742029Z","iopub.execute_input":"2022-05-08T19:15:25.742625Z","iopub.status.idle":"2022-05-08T19:15:25.762842Z","shell.execute_reply.started":"2022-05-08T19:15:25.742568Z","shell.execute_reply":"2022-05-08T19:15:25.761778Z"},"trusted":true},"execution_count":93,"outputs":[]},{"cell_type":"code","source":"lc = learn_curve(x,y,1)\nprint(f'Cross Validation Accuracies:\\n{\"-\"*25}\\n{list(lc[\"cv_scores\"])}\\n\\n\\\nMean Cross Validation Accuracy:\\n{\"-\"*25}\\n{np.mean(lc[\"cv_scores\"])}\\n\\n\\\nStandard Deviation of Cross Validation Accuracy:\\n{\"-\"*25}\\n{np.std(lc[\"cv_scores\"])}\\n\\n\\\nTraining Accuracy:\\n{\"-\"*15}\\n{lc[\"train_score\"]}\\n\\n')\nsns.lineplot(data=lc[\"learning_curve\"],x=\"Training_size\",y=\"value\",hue=\"variable\")\nplt.title(\"Learning Curve of Model\")\nplt.ylabel(\"Misclassification Rate/Loss\");","metadata":{"execution":{"iopub.status.busy":"2022-05-08T19:15:25.765031Z","iopub.execute_input":"2022-05-08T19:15:25.765759Z","iopub.status.idle":"2022-05-08T19:15:53.620292Z","shell.execute_reply.started":"2022-05-08T19:15:25.765706Z","shell.execute_reply":"2022-05-08T19:15:53.619466Z"},"trusted":true},"execution_count":94,"outputs":[]},{"cell_type":"markdown","source":"**Above graph shows the miscalculation v/s training size.\nWe see that as training dataset size increases, validation loss/miscalculation decreases and thus we can infer that we need more dataset for better prediction results**","metadata":{}},{"cell_type":"code","source":"# Create an object of SVC model and fit x,y values in it in order to generate pickle file\nmodel_svm_argby=svm.SVC()\nmodel_svm_argby.fit(x.values,y)","metadata":{"execution":{"iopub.status.busy":"2022-05-08T19:15:53.621418Z","iopub.execute_input":"2022-05-08T19:15:53.621729Z","iopub.status.idle":"2022-05-08T19:15:54.314937Z","shell.execute_reply.started":"2022-05-08T19:15:53.621695Z","shell.execute_reply":"2022-05-08T19:15:54.314350Z"},"trusted":true},"execution_count":95,"outputs":[]},{"cell_type":"code","source":"# Create an object of KNN model and fit x,y values in it in order to generate pickle file\nmodel_knn_argby=KNeighborsClassifier(n_neighbors=5, metric='minkowski', p=2 )  \nmodel_knn_argby.fit(x.values,y)","metadata":{"execution":{"iopub.status.busy":"2022-05-08T19:15:54.316078Z","iopub.execute_input":"2022-05-08T19:15:54.316622Z","iopub.status.idle":"2022-05-08T19:15:54.327135Z","shell.execute_reply.started":"2022-05-08T19:15:54.316587Z","shell.execute_reply":"2022-05-08T19:15:54.326320Z"},"trusted":true},"execution_count":96,"outputs":[]},{"cell_type":"code","source":"# Create an object of GNB model and fit x,y values in it in order to generate pickle file\nmodel_gnb_argby=GaussianNB()\nmodel_gnb_argby.fit(x.values,y)","metadata":{"execution":{"iopub.status.busy":"2022-05-08T19:15:54.328419Z","iopub.execute_input":"2022-05-08T19:15:54.328631Z","iopub.status.idle":"2022-05-08T19:15:54.373909Z","shell.execute_reply.started":"2022-05-08T19:15:54.328605Z","shell.execute_reply":"2022-05-08T19:15:54.373311Z"},"trusted":true},"execution_count":97,"outputs":[]},{"cell_type":"markdown","source":"**GENERATION OF PICKLE FILES FOR ML MODEL**","metadata":{}},{"cell_type":"code","source":"# Creation of pickle file for svm model to predict argument by\nmodel_file = \"model_svm_argby\"\nwith open(model_file,'wb') as f:\n    pickle.dump(model_svm_argby, f)\n    \n# Creation of pickle file for knn model to predict argument by\nmodel_file = \"model_knn_argby\"\nwith open(model_file,'wb') as f:\n    pickle.dump(model_knn_argby, f)\n    \n# Creation of pickle file for gnb model to predict argument by\nmodel_file = \"model_gnb_argby\"\nwith open(model_file,'wb') as f:\n    pickle.dump(model_gnb_argby, f)","metadata":{"execution":{"iopub.status.busy":"2022-05-08T19:15:54.375072Z","iopub.execute_input":"2022-05-08T19:15:54.375433Z","iopub.status.idle":"2022-05-08T19:15:54.443451Z","shell.execute_reply.started":"2022-05-08T19:15:54.375390Z","shell.execute_reply":"2022-05-08T19:15:54.442356Z"},"trusted":true},"execution_count":98,"outputs":[]},{"cell_type":"code","source":"# Creation of pickle file for vectorizer to get the text into vector form for prediction using Machine Learning\nmodel_file=\"model_vect_argby\"\nwith open(model_file,'wb') as f:\n    pickle.dump(vectorizer, f)","metadata":{"execution":{"iopub.status.busy":"2022-05-08T19:15:54.445490Z","iopub.execute_input":"2022-05-08T19:15:54.446092Z","iopub.status.idle":"2022-05-08T19:15:54.451852Z","shell.execute_reply.started":"2022-05-08T19:15:54.446046Z","shell.execute_reply":"2022-05-08T19:15:54.451298Z"},"trusted":true},"execution_count":99,"outputs":[]},{"cell_type":"markdown","source":"**PREDICTING THE ARGUMENT BY FOR GIVEN TEXT**","metadata":{}},{"cell_type":"code","source":"loaded_model_vect = pickle.load(open(\"model_vect_argby\", 'rb'))\nX_vec=loaded_model_vect.transform([\"heard learned advocate mr waghmare for the applicant\"])","metadata":{"execution":{"iopub.status.busy":"2022-05-08T19:15:54.452911Z","iopub.execute_input":"2022-05-08T19:15:54.453674Z","iopub.status.idle":"2022-05-08T19:15:54.466941Z","shell.execute_reply.started":"2022-05-08T19:15:54.453636Z","shell.execute_reply":"2022-05-08T19:15:54.466322Z"},"trusted":true},"execution_count":100,"outputs":[]},{"cell_type":"code","source":"loaded_model_argby = pickle.load(open(\"model_svm_argby\", 'rb'))\nprint(loaded_model_argby.predict(X_vec.toarray()))","metadata":{"execution":{"iopub.status.busy":"2022-05-08T19:15:54.468107Z","iopub.execute_input":"2022-05-08T19:15:54.468363Z","iopub.status.idle":"2022-05-08T19:15:54.486113Z","shell.execute_reply.started":"2022-05-08T19:15:54.468333Z","shell.execute_reply":"2022-05-08T19:15:54.485311Z"},"trusted":true},"execution_count":101,"outputs":[]}]}