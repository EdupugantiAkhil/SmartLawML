{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import json\nimport glob\nimport numpy as np\nimport pandas as pd\nimport csv\nimport string\nimport re\nimport math\nimport sklearn as sk\nimport nltk\nimport heapq\nimport matplotlib.pyplot as plt\nfrom nltk.tokenize import word_tokenize\nfrom nltk.tokenize import sent_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.stem.snowball import SnowballStemmer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom wordcloud import WordCloud\nfrom sklearn.feature_extraction.text import TfidfTransformer# This Python 3 environment comes with many helpful analytics libraries installed\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import LinearSVC\nfrom sklearn.neighbors import KNeighborsClassifier\n\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import learning_curve\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.model_selection import train_test_split\nfrom nltk.corpus import stopwords\nfrom nltk import pos_tag\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import learning_curve\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import StratifiedKFold\nimport seaborn as sns\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-04-26T14:01:19.612434Z","iopub.execute_input":"2022-04-26T14:01:19.613145Z","iopub.status.idle":"2022-04-26T14:01:21.588691Z","shell.execute_reply.started":"2022-04-26T14:01:19.613035Z","shell.execute_reply":"2022-04-26T14:01:21.587822Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"train_files = glob.glob(\"../input/sample1/SmartLawDataset-main/json/*.json\")","metadata":{"execution":{"iopub.status.busy":"2022-04-26T14:01:21.592233Z","iopub.execute_input":"2022-04-26T14:01:21.592520Z","iopub.status.idle":"2022-04-26T14:01:21.614056Z","shell.execute_reply.started":"2022-04-26T14:01:21.592489Z","shell.execute_reply":"2022-04-26T14:01:21.613093Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"1. Need wordset of unique words\n2. sentences without removing anything\n3. 1933 dictionaries","metadata":{}},{"cell_type":"code","source":"mergejson=pd.DataFrame()\nfor file in train_files:\n    with open(file) as f1: \n        data1 = json.load(f1)\n    df1=pd.DataFrame([data1])\n    mergejson=pd.concat([mergejson,df1])\nmg1=mergejson.drop(['header','background','order','footer','annotationProcessingStage','annotationProcessingStageAnnotations','processedText'], axis=1)\nmg1.to_csv('file2.csv')\n#print(mergejson)","metadata":{"execution":{"iopub.status.busy":"2022-04-26T14:01:21.615176Z","iopub.execute_input":"2022-04-26T14:01:21.615516Z","iopub.status.idle":"2022-04-26T14:01:22.161688Z","shell.execute_reply.started":"2022-04-26T14:01:21.615485Z","shell.execute_reply":"2022-04-26T14:01:22.160668Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"header=['ARGUMENT','TYPE']\nfilename = 'final_argument_by.csv'\ndata1=[]\nwith open(filename, 'w') as file:\n    csvwriter = csv.writer(file) # 2. create a csvwriter object\n    csvwriter.writerow(header) # 4. write the header\n    for j in mg1['arguments']:\n        for k in j:           \n            for p in k['argumentSentences']:                \n                data=[]\n                if(p['argumentSentenceType']=='NA'):\n                    continue\n                data.append(p['text'][:])\n                data.append(p['argumentSentenceType'])\n                data1.append(data)\n    csvwriter.writerows(data1) # 5. write the rest of the data\ndataset = pd.read_csv('final_argument_by.csv')\ndf = pd.DataFrame(dataset)\nprint(df)","metadata":{"execution":{"iopub.status.busy":"2022-04-26T14:01:22.164106Z","iopub.execute_input":"2022-04-26T14:01:22.164344Z","iopub.status.idle":"2022-04-26T14:01:22.205983Z","shell.execute_reply.started":"2022-04-26T14:01:22.164315Z","shell.execute_reply":"2022-04-26T14:01:22.205322Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"s =  set(string.punctuation)\ns.add('\\xad')\nfor index, row in df.iterrows():\n    row['ARGUMENT']=row['ARGUMENT'].lower()\n    for x in row['ARGUMENT']:\n        if x in s or re.search(r'-?\\d+', x): \n            row['ARGUMENT']=row['ARGUMENT'].replace(x,\"\").strip()\n#Now we have proper dataset after removing unwanted punctuations etc\narr=[]\narr1=[]\ncount=0\nc1=0\nfor index, row in df.iterrows():\n    if(row['TYPE']=='PREMISE'):\n        count+=1\n        if(count>290):\n            continue\n    arr.append(row['ARGUMENT'])\n    arr1.append(row['TYPE'])\n\ndf1=pd.DataFrame()\ndf1[\"argument\"] = arr\ndf1[\"by\"] = arr1\nprint(df1)","metadata":{"execution":{"iopub.status.busy":"2022-04-26T14:01:22.206936Z","iopub.execute_input":"2022-04-26T14:01:22.207629Z","iopub.status.idle":"2022-04-26T14:01:22.851047Z","shell.execute_reply.started":"2022-04-26T14:01:22.207560Z","shell.execute_reply":"2022-04-26T14:01:22.849883Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# #list of all sentences\nfinal_list=[]\nfor item in df1['argument']:\n    final_list.append(item)\n#print(final_list)","metadata":{"execution":{"iopub.status.busy":"2022-04-26T14:01:22.852728Z","iopub.execute_input":"2022-04-26T14:01:22.853168Z","iopub.status.idle":"2022-04-26T14:01:22.859125Z","shell.execute_reply.started":"2022-04-26T14:01:22.853118Z","shell.execute_reply":"2022-04-26T14:01:22.858224Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"\nvectorizer = TfidfVectorizer()\nvectors = vectorizer.fit_transform(final_list)\nfeature_names = vectorizer.get_feature_names_out()\ndense = vectors.todense()\ndenselist = dense.tolist()\ncolumns=feature_names\nheader=columns\ndf3 = pd.DataFrame(denselist, columns=feature_names) \n# print(columns)\n# print(len(columns))\n# print(len(df3))\n# print(len(arr1))\n\ndf3['TYpe']=df1['by']\ndf3.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-26T14:01:22.860411Z","iopub.execute_input":"2022-04-26T14:01:22.860675Z","iopub.status.idle":"2022-04-26T14:01:23.190342Z","shell.execute_reply.started":"2022-04-26T14:01:22.860642Z","shell.execute_reply":"2022-04-26T14:01:23.189562Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"y=df3.TYpe\nfeatures=header\nx=df3[features]","metadata":{"execution":{"iopub.status.busy":"2022-04-26T14:01:23.191700Z","iopub.execute_input":"2022-04-26T14:01:23.192012Z","iopub.status.idle":"2022-04-26T14:01:23.206721Z","shell.execute_reply.started":"2022-04-26T14:01:23.191966Z","shell.execute_reply":"2022-04-26T14:01:23.204953Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(x, y, test_size = 0.20, random_state = 0)\n# check the shape of X_train and X_test\nX_train.shape, X_test.shape","metadata":{"execution":{"iopub.status.busy":"2022-04-26T14:01:23.208275Z","iopub.execute_input":"2022-04-26T14:01:23.208507Z","iopub.status.idle":"2022-04-26T14:01:23.353748Z","shell.execute_reply.started":"2022-04-26T14:01:23.208476Z","shell.execute_reply":"2022-04-26T14:01:23.352812Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"is tf-idf for full doc or sentence?\nhow to use in model","metadata":{}},{"cell_type":"code","source":"gnb = GaussianNB()\ngnb.fit(X_train, y_train)\n","metadata":{"execution":{"iopub.status.busy":"2022-04-26T14:01:23.356297Z","iopub.execute_input":"2022-04-26T14:01:23.356519Z","iopub.status.idle":"2022-04-26T14:01:23.393248Z","shell.execute_reply.started":"2022-04-26T14:01:23.356491Z","shell.execute_reply":"2022-04-26T14:01:23.392461Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"y_pred = gnb.predict(X_test)\ny_pred_train = gnb.predict(X_train)","metadata":{"execution":{"iopub.status.busy":"2022-04-26T14:01:23.397045Z","iopub.execute_input":"2022-04-26T14:01:23.397925Z","iopub.status.idle":"2022-04-26T14:01:23.441295Z","shell.execute_reply.started":"2022-04-26T14:01:23.397874Z","shell.execute_reply":"2022-04-26T14:01:23.440415Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"print('Model accuracy score: {0:0.4f}'. format(accuracy_score(y_test, y_pred)))","metadata":{"execution":{"iopub.status.busy":"2022-04-26T14:01:23.442362Z","iopub.execute_input":"2022-04-26T14:01:23.443093Z","iopub.status.idle":"2022-04-26T14:01:23.449215Z","shell.execute_reply.started":"2022-04-26T14:01:23.443055Z","shell.execute_reply":"2022-04-26T14:01:23.448184Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"print(y_test.value_counts())\nnull_accuracy = (76/(69+76))\nprint('Null accuracy score: {0:0.4f}'. format(null_accuracy))","metadata":{"execution":{"iopub.status.busy":"2022-04-26T14:01:23.450239Z","iopub.execute_input":"2022-04-26T14:01:23.450453Z","iopub.status.idle":"2022-04-26T14:01:23.468275Z","shell.execute_reply.started":"2022-04-26T14:01:23.450426Z","shell.execute_reply":"2022-04-26T14:01:23.467295Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"#SVC model\nlsvc = LinearSVC(verbose=0)\nprint(lsvc)","metadata":{"execution":{"iopub.status.busy":"2022-04-26T14:01:23.469371Z","iopub.execute_input":"2022-04-26T14:01:23.469989Z","iopub.status.idle":"2022-04-26T14:01:23.479438Z","shell.execute_reply.started":"2022-04-26T14:01:23.469947Z","shell.execute_reply":"2022-04-26T14:01:23.478752Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n          intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n          multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n          verbose=0)","metadata":{"execution":{"iopub.status.busy":"2022-04-26T14:01:23.480662Z","iopub.execute_input":"2022-04-26T14:01:23.480991Z","iopub.status.idle":"2022-04-26T14:01:23.494278Z","shell.execute_reply.started":"2022-04-26T14:01:23.480962Z","shell.execute_reply":"2022-04-26T14:01:23.493645Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"lsvc.fit(X_train, y_train)\nscore = lsvc.score(X_train, y_train)\nprint(\"Score: \", score)","metadata":{"execution":{"iopub.status.busy":"2022-04-26T14:01:23.495378Z","iopub.execute_input":"2022-04-26T14:01:23.495837Z","iopub.status.idle":"2022-04-26T14:01:23.544308Z","shell.execute_reply.started":"2022-04-26T14:01:23.495804Z","shell.execute_reply":"2022-04-26T14:01:23.543344Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"#KNN model\nknn = KNeighborsClassifier(n_neighbors= 5) # k=3\nknn.fit(X_train,y_train)\nprediction = knn.predict(X_test)","metadata":{"execution":{"iopub.status.busy":"2022-04-26T14:01:23.546048Z","iopub.execute_input":"2022-04-26T14:01:23.546628Z","iopub.status.idle":"2022-04-26T14:01:23.620510Z","shell.execute_reply.started":"2022-04-26T14:01:23.546555Z","shell.execute_reply":"2022-04-26T14:01:23.619461Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"print(\"{} nın score: {}\".format(3,knn.score(X_test,y_test)))","metadata":{"execution":{"iopub.status.busy":"2022-04-26T14:01:23.622350Z","iopub.execute_input":"2022-04-26T14:01:23.622991Z","iopub.status.idle":"2022-04-26T14:01:23.670454Z","shell.execute_reply.started":"2022-04-26T14:01:23.622937Z","shell.execute_reply":"2022-04-26T14:01:23.669531Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"def learn_curve(X,y,c):\n    le = preprocessing.LabelEncoder() # Label encoding the target\n    sc = preprocessing.StandardScaler() # Scaling the input features\n    y = le.fit_transform(y)#Label Encoding the target\n    log_reg = LogisticRegression(max_iter=200,random_state=11,C=c)\n    lr = Pipeline(steps=(['scaler',sc],\n                        ['classifier',log_reg]))\n    \n    \n    cv = StratifiedKFold(n_splits=5,random_state=11,shuffle=True) # Creating a StratifiedKFold object with 5 folds\n    cv_scores = cross_val_score(lr,X,y,scoring=\"accuracy\",cv=cv) # Storing the CV scores (accuracy) of each fold\n    lr.fit(X,y) # Fitting the model\n    train_score = lr.score(X,y) # Scoring the model on train set\n    #Building the learning curve\n    train_size,train_scores,test_scores = learning_curve(estimator=lr,X=X,y=y,cv=cv,scoring=\"accuracy\",random_state=11)\n    train_scores = 1-np.mean(train_scores,axis=1)#converting the accuracy score to misclassification rate\n    test_scores = 1-np.mean(test_scores,axis=1)#converting the accuracy score to misclassification rate\n    lc = pd.DataFrame({\"Training_size\":train_size,\"Training_loss\":train_scores,\"Validation_loss\":test_scores}).melt(id_vars=\"Training_size\")\n    return {\"cv_scores\":cv_scores,\n           \"train_score\":train_score,\n           \"learning_curve\":lc}","metadata":{"execution":{"iopub.status.busy":"2022-04-26T14:01:23.672259Z","iopub.execute_input":"2022-04-26T14:01:23.672835Z","iopub.status.idle":"2022-04-26T14:01:23.689396Z","shell.execute_reply.started":"2022-04-26T14:01:23.672782Z","shell.execute_reply":"2022-04-26T14:01:23.688399Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"lc = learn_curve(x,y,1)\nprint(f'Cross Validation Accuracies:\\n{\"-\"*25}\\n{list(lc[\"cv_scores\"])}\\n\\n\\\nMean Cross Validation Accuracy:\\n{\"-\"*25}\\n{np.mean(lc[\"cv_scores\"])}\\n\\n\\\nStandard Deviation of Cross Validation Accuracy:\\n{\"-\"*25}\\n{np.std(lc[\"cv_scores\"])}\\n\\n\\\nTraining Accuracy:\\n{\"-\"*15}\\n{lc[\"train_score\"]}\\n\\n')\nsns.lineplot(data=lc[\"learning_curve\"],x=\"Training_size\",y=\"value\",hue=\"variable\")\nplt.title(\"Learning Curve of Model\")\nplt.ylabel(\"Misclassification Rate/Loss\");","metadata":{"execution":{"iopub.status.busy":"2022-04-26T14:01:23.691142Z","iopub.execute_input":"2022-04-26T14:01:23.691762Z","iopub.status.idle":"2022-04-26T14:01:28.849485Z","shell.execute_reply.started":"2022-04-26T14:01:23.691709Z","shell.execute_reply":"2022-04-26T14:01:28.848474Z"},"trusted":true},"execution_count":20,"outputs":[]}]}